{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1674,
     "status": "ok",
     "timestamp": 1643960294458,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "Le8ek_WSAqcK",
    "outputId": "a2a6206b-2cbf-4449-8acd-de2541c3cc50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kharech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kharech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kharech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kharech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# NLTK libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Using Cosine Similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Import and suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Modelling \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb  # Load this xgboost\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24219,
     "status": "ok",
     "timestamp": 1643960322282,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "UlRhteHNA1r9",
    "outputId": "5064b2fa-85ef-477c-af36-abd195428058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1643961434778,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "zInJHUQ2BAId"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/gdrive/MyDrive/CapstoneProject')  #change dir of working folder\n",
    "# Reading data from the the file \n",
    "data = pd.read_csv('sample30.csv' , encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1643961436843,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "LgwVGoXeAqcO"
   },
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "    # remove html markup\n",
    "    text=re.sub(\"(<.*?>)\",\" \",text)\n",
    "    \n",
    "    # remove unneccessary words\n",
    "    text = text.replace(\"!\",\"\")\n",
    "    text = text.replace(\":\",\"\")\n",
    "    text = text.replace(\"_\",\" \")\n",
    "    \n",
    "    #remove non-ascii and digits\n",
    "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    \n",
    "    #remove whitespace\n",
    "    text = text.strip()\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        #print(word)\n",
    "        #print(tag)\n",
    "        #print(\"***************\")\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            #lemmatized_sentence.append(word)\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        #print(lemmatized_sentence)\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    strip_accents='unicode',    # Remove accents and perform other character normalization during the preprocessing step. \n",
    "    analyzer='word',            # Whether the feature should be made of word or character n-grams.\n",
    "    token_pattern=r'\\w{1,}',    # Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'\n",
    "    ngram_range=(1, 3),         # The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True)\n",
    "\n",
    "# Reading data from the the file \n",
    "data = pd.read_csv('sample30.csv' , encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 100161,
     "status": "ok",
     "timestamp": 1643961952607,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "MH_RiLTKt5HO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>name</th>\n",
       "      <th>reviews_username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love this album . it 's very good . more to ...</td>\n",
       "      <td>Pink Friday: Roman Reloaded Re-Up (w/dvd)</td>\n",
       "      <td>joshua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good flavor . This review be collect a part of...</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>dorothy w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good flavor .</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>dorothy w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I read through the review on here before look ...</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>rebecca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My husband buy this gel for u . The gel cause ...</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>walker557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>I get this conditioner with Influenster to try...</td>\n",
       "      <td>L'or233al Paris Elvive Extraordinary Clay Reba...</td>\n",
       "      <td>laurasnchz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>I love it , I receive this for review purpose ...</td>\n",
       "      <td>L'or233al Paris Elvive Extraordinary Clay Reba...</td>\n",
       "      <td>scarlepadilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>First of all I love the smell of this product ...</td>\n",
       "      <td>L'or233al Paris Elvive Extraordinary Clay Reba...</td>\n",
       "      <td>liviasuexo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>I receive this through Influenster and will ne...</td>\n",
       "      <td>L'or233al Paris Elvive Extraordinary Clay Reba...</td>\n",
       "      <td>ktreed95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>I receive this product complimentary from infl...</td>\n",
       "      <td>L'or233al Paris Elvive Extraordinary Clay Reba...</td>\n",
       "      <td>kcoopxoxo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviews_text  \\\n",
       "0      i love this album . it 's very good . more to ...   \n",
       "1      Good flavor . This review be collect a part of...   \n",
       "2                                          Good flavor .   \n",
       "3      I read through the review on here before look ...   \n",
       "4      My husband buy this gel for u . The gel cause ...   \n",
       "...                                                  ...   \n",
       "29995  I get this conditioner with Influenster to try...   \n",
       "29996  I love it , I receive this for review purpose ...   \n",
       "29997  First of all I love the smell of this product ...   \n",
       "29998  I receive this through Influenster and will ne...   \n",
       "29999  I receive this product complimentary from infl...   \n",
       "\n",
       "                                                    name reviews_username  \n",
       "0              Pink Friday: Roman Reloaded Re-Up (w/dvd)           joshua  \n",
       "1             Lundberg Organic Cinnamon Toast Rice Cakes        dorothy w  \n",
       "2             Lundberg Organic Cinnamon Toast Rice Cakes        dorothy w  \n",
       "3                       K-Y Love Sensuality Pleasure Gel          rebecca  \n",
       "4                       K-Y Love Sensuality Pleasure Gel        walker557  \n",
       "...                                                  ...              ...  \n",
       "29995  L'or233al Paris Elvive Extraordinary Clay Reba...       laurasnchz  \n",
       "29996  L'or233al Paris Elvive Extraordinary Clay Reba...    scarlepadilla  \n",
       "29997  L'or233al Paris Elvive Extraordinary Clay Reba...       liviasuexo  \n",
       "29998  L'or233al Paris Elvive Extraordinary Clay Reba...         ktreed95  \n",
       "29999  L'or233al Paris Elvive Extraordinary Clay Reba...        kcoopxoxo  \n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df = pd.DataFrame()\n",
    "tmp_df['reviews_text']=data['reviews_text'].apply(lambda x: lemmatize_sentence(x))\n",
    "tmp_df['name']=data['name']\n",
    "tmp_df['reviews_username'] = data['reviews_username']\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1132,
     "status": "ok",
     "timestamp": 1643962310590,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "QCpxfvguty4f",
    "outputId": "4858785f-b991-4737-835b-896cfd3e2810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lemmatize_sentence.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tmp_df,  'lemmatize_sentence.pkl',compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1643962314685,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "5NehFDkXty75"
   },
   "outputs": [],
   "source": [
    "lemma_temp = joblib.load('lemmatize_sentence.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 65157,
     "status": "ok",
     "timestamp": 1643960398805,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "9uoEP8Xcpzxu"
   },
   "outputs": [],
   "source": [
    "\n",
    "data.dropna( how='any', subset=['user_sentiment'],inplace=True )\n",
    "data['user_sentiment_updated'] = data['user_sentiment'].map({'Positive':1, 'Negative':0})\n",
    "\n",
    "#converting into string\n",
    "data['reviews_text'] = data['reviews_text'].astype('str')\n",
    "\n",
    "# Remove punctuation \n",
    "data['reviews_text'] = data['reviews_text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Remove Stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['reviews_text'] = data['reviews_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#Converting to lower case\n",
    "data['reviews_text']= data['reviews_text'].str.lower()\n",
    "data['reviews_text']=data['reviews_text'].apply(lambda x: scrub_words(x))\n",
    "data['reviews_text']=data['reviews_text'].apply(lambda x: lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 14421,
     "status": "error",
     "timestamp": 1643955940275,
     "user": {
      "displayName": "Chetan Khare",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg3o6QHMHhFC_2HCoknL9kwW-xQUDbV0peUcP11Kw=s64",
      "userId": "15880889895853742984"
     },
     "user_tz": -330
    },
    "id": "VtFF4KisAqcQ",
    "outputId": "97875dcb-f61d-42b2-8ecd-1a5fac494745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:58:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#x=data['reviews_text'] \n",
    "#y=data['user_sentiment_updated']\n",
    "seed = 50 \n",
    "train, test = train_test_split(data, test_size=0.30, random_state=seed)\n",
    "word_vectorizer.fit(train.reviews_text) \n",
    "## transforming the train and test datasets\n",
    "X_train_transformed = word_vectorizer.transform(train.reviews_text.tolist())\n",
    "X_test_transformed = word_vectorizer.transform(test.reviews_text.tolist())\n",
    "\n",
    "counter = Counter(train.user_sentiment_updated)\n",
    "sm = SMOTE()\n",
    "# transform the dataset\n",
    "X_train_transformed_sm, y_train_sm = sm.fit_resample(X_train_transformed, train.user_sentiment_updated)\n",
    "counter = Counter(y_train_sm)\n",
    "\n",
    "# Building the XGBoost Regularized Regression model\n",
    "\n",
    "xgb_cfl = xgb.XGBClassifier(n_jobs = -1,objective = 'binary:logistic',n_estimators=1000,learning_rate = 0.1, min_child_weight = 1, gamma = 0.1,subsample = 1, colsample_bytree = 1, max_depth = 10)\n",
    "xgb_cfl.fit(X_train_transformed_sm, y_train_sm) \n",
    "\n",
    "# Prediction Train Data\n",
    "y_pred_train_sm= xgb_cfl.predict(X_train_transformed_sm)\n",
    "\n",
    "\n",
    "# Prediction Test Data\n",
    "y_pred_test = xgb_cfl.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W2KOhPwlAqcR"
   },
   "outputs": [],
   "source": [
    "with open ('sentiment_model.pkl','wb') as fp:\n",
    "    pickle.dump(xgb_cfl,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yP6TQZ4yAqcS"
   },
   "outputs": [],
   "source": [
    "df_pivot = train.pivot_table(index ='reviews_username', columns ='name', values ='reviews_rating', aggfunc='count').fillna(0)\n",
    "\n",
    "# Copy the train dataset into dummy_train\n",
    "dummy_train = train.copy()\n",
    "dummy_train = dummy_train.groupby(['reviews_username','name'])['reviews_rating'].count().reset_index()\n",
    "dummy_train['rating'] = dummy_train['reviews_rating'].apply(lambda x: 0 if x>=1 else 1)\n",
    "#dummy_train.head(2)\n",
    "# Convert the dummy train dataset into matrix format.\n",
    "dummy_train = dummy_train.pivot(\n",
    "    index='reviews_username',\n",
    "    columns='name',\n",
    "    values='rating'\n",
    ").fillna(1)\n",
    "\n",
    "user_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "user_correlation[user_correlation<0]=0\n",
    "user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))\n",
    "user_final_rating = np.multiply(user_predicted_ratings,dummy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1633676051637,
     "user": {
      "displayName": "shashank lal",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05906478430139842478"
     },
     "user_tz": -330
    },
    "id": "VGBHrVmeZ6Oo",
    "outputId": "f1602f78-a5fa-4642-91a5-ba7fb9564954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recommend_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(user_final_rating,  'recommend_model.pkl',compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EjL5ynqTrxLC"
   },
   "outputs": [],
   "source": [
    "recommend_model = joblib.load('recommend_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HhvFBToJLx7W"
   },
   "outputs": [],
   "source": [
    "with open ('word_vectorizer.pkl','wb') as fp:\n",
    "    pickle.dump(word_vectorizer,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nAfVaiAELzjE"
   },
   "outputs": [],
   "source": [
    "with open ('word_vectorizer.pkl','rb') as f:\n",
    "    word_vectorizer = pickle.load(f)\n",
    "\n",
    "with open ('sentiment_model.pkl','rb') as f:\n",
    "    sentiment_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1633676143400,
     "user": {
      "displayName": "shashank lal",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05906478430139842478"
     },
     "user_tz": -330
    },
    "id": "8qz239eqAqcS",
    "outputId": "83b1a07d-81a2-4536-a07b-60dae312f22a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>Positive Sentiment(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clorox Disinfecting Wipes Value Pack Scented 1...</td>\n",
       "      <td>80.147039</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Resident Evil Collection 5 Discs (blu-Ray)</td>\n",
       "      <td>28.483991</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Di...</td>\n",
       "      <td>24.894786</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red (special Edition) (dvdvideo)</td>\n",
       "      <td>12.962281</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nexxus Exxtra Gel Style Creation Sculptor</td>\n",
       "      <td>8.433311</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name      score  \\\n",
       "0  Clorox Disinfecting Wipes Value Pack Scented 1...  80.147039   \n",
       "1     The Resident Evil Collection 5 Discs (blu-Ray)  28.483991   \n",
       "2  My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Di...  24.894786   \n",
       "3                   Red (special Edition) (dvdvideo)  12.962281   \n",
       "4          Nexxus Exxtra Gel Style Creation Sculptor   8.433311   \n",
       "\n",
       "  Positive Sentiment(%)  \n",
       "0                        \n",
       "1                        \n",
       "2                        \n",
       "3                        \n",
       "4                        "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = 'tony'\n",
    "temp = pd.DataFrame(recommend_model.loc[user_input].sort_values(ascending=False))\n",
    "if temp.shape[0] > 20:\n",
    "    top20 =  temp[0:20]\n",
    "else:\n",
    "    top20 =  temp\n",
    "    \n",
    "top20 = top20.reset_index()\n",
    "top20.rename(columns = {user_input:'score'}, inplace = True)\n",
    "top20.insert(2, \"Positive Sentiment(%)\", \"\") \n",
    "top20.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UT0-SJ0UrMyk"
   },
   "outputs": [],
   "source": [
    "for prod in top20['name']:\n",
    "    #print(prod)\n",
    "    rev = data[data['name'] == prod]\n",
    "    if rev.shape[0] > 0:\n",
    "        temp = rev['reviews_text'].apply(lambda x: lemmatize_sentence(x))\n",
    "        temp1 = word_vectorizer.transform(temp)\n",
    "        temp2 = sentiment_model.predict(temp1)\n",
    "        pos = sum(temp2)\n",
    "        total = len(temp2)\n",
    "        percent = round(pos*100/total,2)\n",
    "        top20.loc[top20['name'] == prod, ['Positive Sentiment(%)']] = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1633676201446,
     "user": {
      "displayName": "shashank lal",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05906478430139842478"
     },
     "user_tz": -330
    },
    "id": "n9aY4S2uAqcU",
    "outputId": "09f2ab18-ce4b-4dd6-c6fc-0445381beb8a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>Positive Sentiment(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stargate (ws) (ultimate Edition) (director's C...</td>\n",
       "      <td>4.68</td>\n",
       "      <td>97.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Di...</td>\n",
       "      <td>24.89</td>\n",
       "      <td>95.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red (special Edition) (dvdvideo)</td>\n",
       "      <td>12.96</td>\n",
       "      <td>94.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Coty Airspun Face Powder, Translucent Extra Co...</td>\n",
       "      <td>3.15</td>\n",
       "      <td>91.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There's Something About Mary (dvd)</td>\n",
       "      <td>5.50</td>\n",
       "      <td>90.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  score  \\\n",
       "15  Stargate (ws) (ultimate Edition) (director's C...   4.68   \n",
       "2   My Big Fat Greek Wedding 2 (blu-Ray + Dvd + Di...  24.89   \n",
       "3                    Red (special Edition) (dvdvideo)  12.96   \n",
       "18  Coty Airspun Face Powder, Translucent Extra Co...   3.15   \n",
       "9                  There's Something About Mary (dvd)   5.50   \n",
       "\n",
       "   Positive Sentiment(%)  \n",
       "15                 97.31  \n",
       "2                  95.51  \n",
       "3                  94.05  \n",
       "18                 91.14  \n",
       "9                  90.77  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = top20.sort_values(by=\"Positive Sentiment(%)\",ascending=False)#[0:5]\n",
    "if temp.shape[0] > 5:\n",
    "    top5 =  temp[0:5]\n",
    "else:\n",
    "    top5 =  temp\n",
    "#top5.shape\n",
    "top5['score'] = round(top5['score'],2)\n",
    "top5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u3Spx_RAqcV"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhuGShp1LkyD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
